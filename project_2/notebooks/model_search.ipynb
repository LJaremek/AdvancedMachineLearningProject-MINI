{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wpartycja/.local/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [20, 10, 99] before, using random point [17, 5, 65]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "Best Recall Score: 0.7205707305583529\n",
      "Best Params: OrderedDict([('max_depth', 20), ('min_samples_split', 10), ('n_estimators', 99)])\n",
      "------------------------------\n",
      "Model: XGBoost\n",
      "Best Recall Score: 0.7149460809219426\n",
      "Best Params: OrderedDict([('learning_rate', 0.01), ('max_depth', 20), ('n_estimators', 67), ('subsample', 0.5)])\n",
      "------------------------------\n",
      "Model: LogisticRegression\n",
      "Best Recall Score: 0.4901562975032891\n",
      "Best Params: OrderedDict([('C', 3932.2516133086), ('penalty', 'l2'), ('solver', 'saga')])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tqdm\n",
    "# Define models and their hyperparameter search spaces\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': Integer(10, 100),\n",
    "            'max_depth': Integer(3, 20),\n",
    "            'min_samples_split': Integer(2, 10)\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'n_estimators': Integer(10, 100),\n",
    "            'max_depth': Integer(3, 20),\n",
    "            'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "            'subsample': Real(0.5, 1.0)\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=10000),\n",
    "        'params': {\n",
    "            'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "            'penalty': Categorical(['l2']),\n",
    "            'solver': Categorical(['lbfgs', 'saga'])\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "            'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "            'gamma': Real(1e-6, 1e+1, prior='log-uniform')\n",
    "        }\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {\n",
    "            'var_smoothing': Real(1e-9, 1e-7, prior='log-uniform')\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Custom scorer for recall\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Dummy dataset (replace with your actual dataset)\n",
    "X = np.loadtxt('boruta_10.txt', delimiter=',')\n",
    "y = np.loadtxt(\"../data/y_train.txt\", delimiter=' ')\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    model = model_info['model']\n",
    "    params = model_info['params']\n",
    "    \n",
    "    # Bayesian optimization with cross-validation\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=model,\n",
    "        search_spaces=params,\n",
    "        scoring=recall_scorer,\n",
    "        cv=kf,\n",
    "        n_iter=30,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    opt.fit(X, y)\n",
    "    \n",
    "    best_results[name] = {\n",
    "        'best_score': opt.best_score_,\n",
    "        'best_params': opt.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Best Recall Score: {opt.best_score_}\")\n",
    "    print(f\"Best Params: {opt.best_params_}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Print overall best results\n",
    "print(\"\\nOverall Best Results:\")\n",
    "for model_name, result in best_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Recall Score: {result['best_score']}\")\n",
    "    print(f\"Best Params: {result['best_params']}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
